{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d82e7a-f788-4cce-a3fd-baafebf4f803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf48a1-5949-4a1e-b883-101244e0b5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "with open(\"environment.yml\") as file_handle:\n",
    "    environment_data = yaml.safe_load(file_handle)\n",
    "\n",
    "for dependency in environment_data[\"dependencies\"]:\n",
    "    if isinstance(dependency, dict):\n",
    "      for lib in dependency['pip']:\n",
    "        os.system(f\"pip install {lib}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce06da-4cd6-4e3c-a5ac-c77eaa8f5628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60af17-8d19-4bc6-82ce-912b1a7be857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f17a6-a195-422f-b734-d35d17620c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true' # or 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80822b3f-a02a-4ea3-81ba-721dad9dbec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv ./kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c09ba6-db02-4bcf-b9b8-2ca358485891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must sign up for the competition and verify your account with a text message before downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9e2ad-9a14-4e40-a980-6a32f82650d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kaggle competitions download -c 'shopee-product-matching'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa027a8-ffa3-4ada-9b7c-961485ea5149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -q -o ./shopee-product-matching.zip -d ./shopee-product-matching/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca77946-d018-4230-ae07-7ff4bdba158c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from modules.datasets.ImageShopeeDataset import ImageShopeeDataset\n",
    "from modules.datasets.TextShopeeDataset import TextShopeeDataset\n",
    "from modules.models.ResNet18EmbeddingsShopeeNet import ResNet18EmbeddingsShopeeNet\n",
    "from modules.models.BERTPreTrainedEmbeddingsShopeeNet import BERTPreTrainedEmbeddingsShopeeNet\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e897b6dc-a7e4-484a-92ea-d031401b9030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './shopee-product-matching/'\n",
    "\n",
    "IS_TEST = False\n",
    "\n",
    "def get_dataset(root=DATA_FOLDER, is_test=False):\n",
    "    name = \"test.csv\" if is_test else \"train.csv\"\n",
    "    df = pd.read_csv(DATA_FOLDER + name)\n",
    "    images_folder = \"test_images/\" if is_test else \"train_images/\"\n",
    "    df['image'] = DATA_FOLDER + images_folder + df['image']\n",
    "    return df\n",
    "\n",
    "def add_target(df):\n",
    "    grouped = df.groupby('label_group')['posting_id'].apply(list)\n",
    "    target = df['label_group'].map(grouped)\n",
    "    new_df = df.copy()\n",
    "    new_df['target'] = target\n",
    "    return new_df\n",
    "\n",
    "df = get_dataset(is_test=IS_TEST)\n",
    "df = add_target(df)\n",
    "train = df[df['image'].apply(lambda x: os.path.exists(x))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97ce5d7c-138c-450c-bcc3-646cf60d17cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare DataLoaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text_dataset = TextShopeeDataset(train['title'].values)\n",
    "\n",
    "def my_collate_fn(data):\n",
    "    \"\"\"\n",
    "    data: list of input text strings\n",
    "    return: dict with keys: input_ids, token_type_ids, attention_mask where each key is a tensor\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    inputs = tokenizer(data, padding=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "    \n",
    "text_loader = torch.utils.data.DataLoader(\n",
    "    text_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=my_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370633d-d14f-42b6-959e-396353bf56ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b93932-9951-4e7d-a408-2343441965da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 68%|██████▊   | 362/536 [01:01<00:24,  7.02it/s]"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda'\n",
    "bert_pretrain_model = BERTPreTrainedEmbeddingsShopeeNet()\n",
    "bert_pretrain_model.to(DEVICE)\n",
    "\n",
    "text_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(text_loader):\n",
    "        \n",
    "        # Check if a GPU is available\n",
    "        if torch.cuda.is_available():\n",
    "            # Move the inputs to the GPU\n",
    "            data = {name: tensor.to('cuda') for name, tensor in data.items()}\n",
    "        else:\n",
    "            print('GPU not available')\n",
    "        \n",
    "        embeddings = bert_pretrain_model(data)\n",
    "        embeddings = embeddings.data.cpu().numpy()\n",
    "        text_embeddings.extend(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5921bf-4776-436b-ae4e-4c98c05ea300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_embeddings = np.stack(text_embeddings)\n",
    "norms = np.linalg.norm(text_embeddings, axis=1)\n",
    "text_embeddings = text_embeddings / norms[:, np.newaxis]\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248370c-e021-4530-88ac-efc674208b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./text_embeddings', text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b4728-e22b-43cd-9595-681d48d86026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install libomp-dev\n",
    "!python -m pip install --upgrade faiss-gpu==1.7.2\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cf700-20f7-42ac-bbf9-5179e0a4f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "res = faiss.StandardGpuResources()\n",
    "index_text = faiss.IndexFlatIP(text_embeddings.shape[1])\n",
    "# index_img = faiss.index_cpu_to_gpu(res, 0, index_img)\n",
    "index_text.add(text_embeddings)\n",
    "similarities_text, indexes_text = index_text.search(text_embeddings, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419debf1-56b9-4ea3-bee9-2b5386c59a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_f1_score(targets, results):\n",
    "    intersect = len(np.intersect1d(targets, results))\n",
    "    return 2 * intersect / (len(targets) + len(results))\n",
    "\n",
    "\n",
    "def process_for_threshold(similarities, indexes, threshold, embeddings):\n",
    "    f1_score_accumulated = 0\n",
    "    for i in range(len(embeddings)):\n",
    "        cur_sims = similarities[i]\n",
    "        cur_indexes = indexes[i]\n",
    "        duplicate_indexes = cur_indexes[cur_sims >= threshold]\n",
    "        results = df.iloc[duplicate_indexes]['posting_id'].values\n",
    "        targets = df.iloc[i]['target']\n",
    "        f1_score = calc_f1_score(targets, results)\n",
    "        f1_score_accumulated += f1_score\n",
    "    return f1_score_accumulated / len(embeddings)\n",
    "\n",
    "\n",
    "thresholds = np.arange(0.85, 0.99, 0.01)\n",
    "f1_avg_scores = []\n",
    "for threshold in tqdm(thresholds):\n",
    "    f1_avg = process_for_threshold(similarities_text, indexes_text, threshold, text_embeddings)\n",
    "    f1_avg_scores.append(f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238be5c-1b85-454f-bdd5-06ba3c1237d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(thresholds, f1_avg_scores)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Threshold for cosine similarity')\n",
    "plt.ylabel('Average F1-score')\n",
    "plt.title('F1-score vs threshold for cosine image similarity')\n",
    "plt.grid(True)\n",
    "\n",
    "max_f1 = max(f1_avg_scores)\n",
    "max_threshold = thresholds[np.argmax(f1_avg_scores)]\n",
    "\n",
    "# plt.annotate(f\"{max_f1}\", xy=(max_threshold, max_f1), xytext=(max_threshold, max_f1-0.05),\n",
    "#              arrowprops=dict(facecolor='black', shrink=0.2), horizontalalignment = 'center')\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('cnn-baseline-thresholds.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010698e4-5e64-4e5c-9bc8-5b70fdb23760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max f1-score: {max_f1}, threshold: {max_threshold}\") "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Base Python 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-base-python-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
